{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 positional encoding\n",
    "$PE(pos,2i) = \\sin(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE(pos,2i+1) = \\cos(pos/10000^{2i/d_{model}})$\n",
    "\n",
    "$PE(pos+k,2i) = \\sin(pos/10000^{2i/d_{model}})\\cos(k/10000^{2i/d_{model}})+ \\cos(pos/10000^{2i/d_{model}})\\sin(k/10000^{2i/d_{model}}) = PE(pos,2i)PE(k,2i+1)+PE(pos,2i+1)PE(k,2i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [3],\n",
       "       [5],\n",
       "       [7],\n",
       "       [9]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(10)[:,np.newaxis]\n",
    "a[1::2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i_dim, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i_dim//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],np.arange(d_model)[np.newaxis, :],d_model)\n",
    "    \n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) #even indicies\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) #odd indicies\n",
    "    \n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return torch.tensor(pos_encoding,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, d = 10, 16\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0]\n",
    "\n",
    "# Juggle the dimensions for the plot\n",
    "pos_encoding = torch.reshape(pos_encoding, (n, d//2, 2))\n",
    "pos_encoding = torch.transpose(pos_encoding,2,0)\n",
    "pos_encoding = torch.reshape(pos_encoding, (d, n))\n",
    "\n",
    "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = torch.tensor(torch.eq(seq,torch.tensor(0)), dtype=torch.float32)\n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    return seq[:, np.newaxis, np.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-180-1ebe6edd47a9>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  seq = torch.tensor(torch.eq(seq,torch.tensor(0)), dtype=torch.float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0., 1., 1.]]],\n",
       "\n",
       "\n",
       "        [[[1., 1., 1., 0., 0.]]]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Scaled dot product attention\n",
    "dimenstion of Q, K, V matrices:\n",
    "* q: query shape == (..., seq_len_q, depth)\n",
    "* k: key shape == (..., seq_len_k, depth)\n",
    "* v: value shape == (..., seq_len_v, depth_v)\n",
    "\n",
    "requirements:\n",
    "* q, k, v must have matching leading dimensions.\n",
    "* k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "* The mask has different shapes depending on its type(padding or look ahead) but it must be broadcastable for addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar = torch.rand(3, 4)\n",
    "ar.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = torch.matmul(q, torch.transpose(k,-2,-1))  # (..., seq_len_q, seq_len_k)\n",
    "    \n",
    "    # scale matmul_qk\n",
    "    dk = torch.tensor(k.shape[-1], dtype=torch.float32)\n",
    "    scaled_attention_logits = matmul_qk/torch.sqrt(dk)\n",
    "    \n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "    \n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores add up to 1.\n",
    "    m = nn.Softmax(dim=1)\n",
    "    attention_weights = m(scaled_attention_logits)\n",
    "    \n",
    "    output = torch.matmul(attention_weights, v) # (..., seq_len_q, depth_v)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test scaled dot product attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[8.4333e-26, 1.0000e+00, 8.4333e-26, 8.4333e-26]])\n",
      "Output is:\n",
      "tensor([[1.0000e+01, 9.2766e-25]])\n"
     ]
    }
   ],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(q, k, v, None)\n",
    "    print('Attention weights are:')\n",
    "    print(temp_attn)\n",
    "    print('Output is:')\n",
    "    print(temp_out)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "temp_k = torch.tensor([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]], dtype=torch.float32)  # (4, 3)\n",
    "\n",
    "temp_v = torch.tensor([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]], dtype=torch.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`, so the second `value` is returned.\n",
    "temp_q = torch.tensor([[0, 10, 0]], dtype=torch.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[4.2166e-26, 4.2166e-26, 5.0000e-01, 5.0000e-01]])\n",
      "Output is:\n",
      "tensor([[550.0000,   5.5000]])\n"
     ]
    }
   ],
   "source": [
    "# This query aligns with a repeated key (third and fourth), so all associated values get averaged.\n",
    "temp_q = torch.tensor([[0, 0, 10]], dtype=torch.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[5.0000e-01, 5.0000e-01, 4.2166e-26, 4.2166e-26]])\n",
      "Output is:\n",
      "tensor([[5.5000e+00, 4.6383e-25]])\n"
     ]
    }
   ],
   "source": [
    "# This query aligns equally with the first and second key, so their values get averaged.\n",
    "temp_q = torch.tensor([[10, 10, 0]], dtype=torch.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tensor([[4.2166e-26, 4.2166e-26, 5.0000e-01, 5.0000e-01],\n",
      "        [8.4333e-26, 1.0000e+00, 8.4333e-26, 8.4333e-26],\n",
      "        [5.0000e-01, 5.0000e-01, 4.2166e-26, 4.2166e-26]])\n",
      "Output is:\n",
      "tensor([[5.5000e+02, 5.5000e+00],\n",
      "        [1.0000e+01, 9.2766e-25],\n",
      "        [5.5000e+00, 4.6383e-25]])\n"
     ]
    }
   ],
   "source": [
    "temp_q = torch.tensor([[0, 0, 10],\n",
    "                      [0, 10, 0],\n",
    "                      [10, 10, 0]], dtype=torch.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        assert d_model % self.num_heads == 0\n",
    "        \n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = q.shape[0]\n",
    "\n",
    "        q = self.wq(q).view(batch_size, -1, self.num_heads, self.depth)\n",
    "        k = self.wk(k).view(batch_size, -1, self.num_heads, self.depth)\n",
    "        v = self.wv(v).view(batch_size, -1, self.num_heads, self.depth)\n",
    "        \n",
    "        k = k.transpose(1,2)\n",
    "        q = q.transpose(1,2)\n",
    "        v = v.transpose(1,2)\n",
    "        \n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = scaled_attention.transpose(1,2)\n",
    "        concat_attention = torch.reshape(scaled_attention, (batch_size, -1, self.d_model))# (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 60, 512]), torch.Size([1, 8, 60, 60]))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = torch.rand(1, 60, 512) # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha.call(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Point wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class point_wise_feed_forward_network(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__() \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test point wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 512])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(torch.rand((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, d_model, eps = 1e-6):\n",
    "        super().__init__()\n",
    "        self.size = d_model\n",
    "        self.alpha = nn.Parameter(torch.ones(self.size))\n",
    "        self.bias = nn.Parameter(torch.zeros(self.size))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) \\\n",
    "        / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 Encoder layer and decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha.call(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 43, 512])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_encoder_layer_output = sample_encoder_layer.call(torch.rand((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layernorm2 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.layernorm3 = torch.nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.dropout1 = torch.nn.Dropout(rate)\n",
    "        self.dropout2 = torch.nn.Dropout(rate)\n",
    "        self.dropout3 = torch.nn.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1.call(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "        attn2, attn_weights_block2 = self.mha2.call(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "        \n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 50, 512])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer.call(torch.rand((64, 50, 512)), sample_encoder_layer_output,False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding,self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i].call(x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-198-b836d8e0e9da>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp_input = torch.tensor(torch.rand((64, 62), dtype=torch.float32)*200, dtype=torch.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 62, 512])\n"
     ]
    }
   ],
   "source": [
    "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,dff=2048, input_vocab_size=8500,maximum_position_encoding=10000)\n",
    "temp_input = torch.tensor(torch.rand((64, 62), dtype=torch.float32)*200, dtype=torch.int64)\n",
    "sample_encoder_output = sample_encoder.call(temp_input, training=False, mask=None)\n",
    "\n",
    "print(sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = torch.nn.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training,look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i].call(x, enc_output, training,look_ahead_mask, padding_mask)\n",
    "            attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
    "\n",
    "            # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-200-df9cce0d19fc>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp_input = torch.tensor(torch.rand((64, 26), dtype=torch.float32)*200, dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 26, 512]), torch.Size([64, 8, 26, 62]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,dff=2048, target_vocab_size=8000,maximum_position_encoding=5000)\n",
    "temp_input = torch.tensor(torch.rand((64, 26), dtype=torch.float32)*200, dtype=torch.int64)\n",
    "output, attn = sample_decoder.call(temp_input,enc_output=sample_encoder_output,training=False,look_ahead_mask=None,padding_mask=None)\n",
    "output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
    "        \n",
    "        self.final_layer = nn.Linear(d_model,target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
    "        \n",
    "        enc_output = self.tokenizer.call(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder.call(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-204-c5b6f04b9873>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp_input = torch.tensor(torch.rand((64, 38), dtype=torch.float32)*200, dtype=torch.int64)\n",
      "<ipython-input-204-c5b6f04b9873>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp_target = torch.tensor(torch.rand((64, 36), dtype=torch.float32)*200, dtype=torch.int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 36, 8000])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(num_layers=2, d_model=512, num_heads=8, dff=2048,input_vocab_size=8500, target_vocab_size=8000,pe_input=10000, pe_target=6000)\n",
    "\n",
    "temp_input = torch.tensor(torch.rand((64, 38), dtype=torch.float32)*200, dtype=torch.int64)\n",
    "temp_target = torch.tensor(torch.rand((64, 36), dtype=torch.float32)*200, dtype=torch.int64)\n",
    "\n",
    "fn_out, _ = sample_transformer.call(temp_input, temp_target, training=False,enc_padding_mask=None,look_ahead_mask=None,dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
